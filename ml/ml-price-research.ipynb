#%% md

# Machine Learning - Price Prediction
The goal is to have a finished model that takes in some features (possibily rarity) and return a predicted price it would sale price. This will hopefully give us a better estimate for what prices these NFT's should be sold at. 

#%% md

## Data Collection 
Collecting the data we will be using the opensea API, this API has three objects we can fetch:

** Asset Object ** 
- These represent NFT's that are available on the opensea marketplace. Here are some popular values:
    - token id: The token id is ordered from 1 - number of items in collection
    - name: The name of the asset
    - traits: The traits the particular object has, these determine the rarity of the traits.
    - last_sale: returns information about the last sale, 
        - total_prive: returns the cost of the total prive raised to the power of 10^18 (/10.\**18)

** Event Object **
- These represent state changes for assets, i.e. if they are on sale. 

** Account Object ** 
- These represent account information. Here are some popular values:
    - address: the wallet generated that uniquely identifies the account.
    - user: the username.
    
The first thing we'd like to do is collect all the information about a single asset.

#%%

## Data Collection for a single asset:
import requests
import json

# The limit is capped at 50
url = "https://api.opensea.io/api/v1/assets"
querystring = {"collection":"cryptopunks", 
               "limit": 1}

headers = {"Accept": "application/json"}

response = requests.request("GET", url, headers=headers, params=querystring)

json.loads(response.text)

#%% md

### Feature Analysis 
Some of the features that seem relevant to each sale are the following:

- token_id
- num_sales
- name
- description
- traits 
- last_sale
- top_bid
- sell_orders
- image_url

This will allow us to isolate certain parameters for a single asset that we can then corrolate to the price that those NFT's have sold for.  

Things to store in the database:
- name 
- description
- num_sales
- last_sale 
- rarity 
- top_bid
- image_url
- artists_id       <- information about artists
- collection_id    <- information about collection  


#%%

def isolate_parameters(asset: dict) -> dict:
    return {
            "num_sales":asset["num_sales"], 
            "name": asset["name"], 
            "description": asset["description"],
            "traits": asset["traits"], 
            "last_sale": asset["last_sale"], 
            "top_bid": asset["top_bid"],
            "sell_orders": asset["sell_orders"],
            "image_url": asset["image_url"]
           }

#%%

isolate_parameters(json.loads(response.text)["assets"][0])

#%% md

Now we need to figure out how to collect asset information about the whole collection. I will start by searching for token_id's from [1:50] and repeat this until the request fails. When the request fails I will stop trying.

#%%

# Collection Assets 
def collect_asset_information(collection_name = "cryptopunks", isolate_parameters=isolate_parameters):
    url = "https://api.opensea.io/api/v1/assets"
    result = []

    for i in range(1000):
        try:
            querystring = {
                "token_ids": list(range((i*29)+1, (i*29)+30)),
                "order_direction":"asc",
                "offset":"0",
                "limit":"50",
                "collection":collection_name,
            }
            headers = {"Accept": "application/json"}

            response = requests.request("GET", url, headers=headers, params=querystring)
            re = json.loads(response.text)
            if re["assets"] == []:
                break;
            for i in re["assets"]:
                # Populate database 
                result.append(isolate_parameters(i))
        except: 
            break;
        
    return result
result = collect_asset_information()

#%%

result

#%%

len(result)

#%% md

## Data-Pre-Processing 

Okay so that's the data collection done for all the assets in a collection regardless of it's size. Let's create a dictionary of traits, and add token id to them to help visualise what affect the traits have. We'll be able to see which traits are more common and which are less. This will help up analyse the pricing of assets of a given trait and their sale price. 

The objects we will be making are TraitData() objects. This will allow us to group traits together isolating them from the asset groups. 
It will also give us information about traits.

#%%

traits_tokens = {}
traits_ids = {}

class TraitData:
    def __init__(self, total_in_set, tokens = [], sold=[], rarity=0):
        self.tokens = tokens
        self.sold = sold 
        self.n = len(tokens)
        self.total = sum([i/10.**18 for i in sold])
        self.total_in_set = total_in_set
        self.rarity = total_in_set
        
    def add_token(self, token_id, total_price=0):
        self.sold.append(total_price/10.**18)
        self.total += total_price/10.**18
        self.tokens.append(token_id)
        self.n+=1
        self.rarity = (self.n / self.total_in_set) * 100
        
    def get_average(self):
        return int(self.total / len(self.sold))
    
    def __str__(self):
        return "average price: " + str(self.get_average()) + "\n" + "rarity: " + str(self.rarity )
        
    def __print__(self):
        print(self)
        

        
for i in result:
    for j in i["traits"]:
        trait_type = j["trait_type"]
        if trait_type in traits_tokens:
            if j["value"] in traits_tokens[trait_type]:
                trait_data = traits_tokens[trait_type][j["value"]]
                if i["last_sale"]:
                    trait_data.add_token(i["token_id"], total_price=int(i["last_sale"]["total_price"]))
                else:
                    trait_data.add_token(i["token_id"])
            else:
                if i["last_sale"]:
                    traits_tokens[trait_type][j["value"]] = TraitData(total_in_set=10000, tokens=[i["token_id"]], sold=[int(i["last_sale"]["total_price"])]) 
                else:
                    traits_tokens[trait_type][j["value"]] = TraitData(total_in_set=10000, tokens=[i["token_id"]]) 
        else:
            if i["last_sale"]:
                traits_tokens[trait_type] = {j["value"]: TraitData(total_in_set=10000, tokens=[i["token_id"]], sold=[int(i["last_sale"]["total_price"])]) }
            else:
                traits_tokens[trait_type] = {j["value"]: TraitData(total_in_set=10000, tokens=[i["token_id"]])}
traits_tokens         

#%%

print(traits_tokens["type"]["Female"])

#%% md

## Data Visualisation

Now let's visualise the data that we get:

#%%

graph_plots = []
for i in traits_tokens:
    for j in traits_tokens[i]:
        print("Characteristic: " + j)
        print(traits_tokens[i][j])
        graph_plots.append(traits_tokens[i][j])

#%%

%matplotlib notebook
import matplotlib.pyplot as plt

plt.plot([i.rarity for i in graph_plots],[i.get_average() for i in graph_plots], 'go-')

#%%



#%%

%matplotlib notebook
import numpy as np

x = np.array([i.rarity for i in graph_plots])
y = np.array([i.get_average() for i in graph_plots])
a, b, c, d = np.polyfit(x, y, 3)

plt.plot(x, y, 'o')
plt.plot(x, a*x**3 + b*x**2 + c*x + d)

#%%



#%%

import pandas as pd 
df = pd.DataFrame(np.array([[i.rarity, i.get_average()] for i in graph_plots]), columns=["rarity", "average-price"])
df

#%%

from sklearn.model_selection import train_test_split
train , test = train_test_split(df, test_size = 0.2) # using a test-size of 0.2 

x_train = train.drop('rarity', axis=1)
y_train = train['average-price']

x_test = test.drop('rarity', axis = 1)
y_test = test['average-price']
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

x_train_scaled = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train_scaled)

x_test_scaled = scaler.fit_transform(x_test)
x_test = pd.DataFrame(x_test_scaled)

#%%

# scaling the features
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

x_train_scaled = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train_scaled)

x_test_scaled = scaler.fit_transform(x_test)
x_test = pd.DataFrame(x_test_scaled)

#%%

#import required packages
from sklearn import neighbors, linear_model
from sklearn.metrics import mean_squared_error 
from math import sqrt
import matplotlib.pyplot as plt
%matplotlib inline

#%%

# So we need to make the model now, and since we have 


# Getting the (Root Mean Squared) deviation for different models:
rmse_val_model0 = [] # linear regression 
rmse_val_model1 = [] # non weighted 
rmse_val_model2 = [] # weighted on distance


print("-------- Model 0 : Linear Model ---------")
    
# Using a KNN Regression inverse distance normal model 
model1 = linear_model.LinearRegression()  
model1.fit(x_train, y_train) 
pred=model1.predict(x_test) 
error = sqrt(mean_squared_error(y_test,pred)) 
rmse_val_model0.append(error) 
print('(Model 0) Deviation', 'is:', error)
    


print("-------- Model 1 : Normal Weighting ---------")
    
# Using a KNN Regression inverse distance normal model 
for K in range(30):
    K = K+1
    model1 = neighbors.KNeighborsRegressor(n_neighbors = K)  
    model1.fit(x_train, y_train) 
    pred=model1.predict(x_test) 
    error = sqrt(mean_squared_error(y_test,pred)) 
    rmse_val_model1.append(error) 
    print('(Model 1) Deviation for k= ' , K , 'is:', error)
    
print("-------- Model 2 : Distance Weighting ---------")
    
# Using a KNN Regression inverse distance weighted model 
for K in range(30):
    K = K+1
    model1 = neighbors.KNeighborsRegressor(n_neighbors = K, weights="distance")  # quite 
    model1.fit(x_train, y_train)  
    pred=model1.predict(x_test) 
    error = sqrt(mean_squared_error(y_test,pred)) 
    rmse_val_model2.append(error) 
    print('(Model 2) Deviation for k=' , K , 'is:', error)

#%%

curve0 = pd.DataFrame([rmse_val_model0 for _ in range(30)]) # single point 
curve1 = pd.DataFrame(rmse_val_model1) # elbow curve  with the best results at k = 7 ?
curve2 = pd.DataFrame(rmse_val_model2) # logarithmic curve with best at k = 1 

curve0.plot()
curve1.plot()
curve2.plot()

#%%

model = neighbors.KNeighborsRegressor(n_neighbors = 10, weights="distance") 
model.fit(x_train, y_train)  
model.predict(np.array([[i] for i in range(100)]))

#%% md

# Pricing Model - Rarity Only
Updating the rarity to compound instead of being individual 

Previous model - Takes each trait and uses the trait rarity to predict price.
i.e. a trait: 
* 1) Headphone - 20% 
* 2) Glasses - 15% 

The sale price will update Headphones and Glasses as a trait. 

This Model - For each asset uses the collective rarity i.e. an image has two traits,
* 1) Headphone - 20% 
* 2) Glasses - 15% 

The final rarity would be 20% x 15% = 3% 

#%%

result = collect_asset_information()

#%%

# this converts each asset in result to a mapping -> [rarity, price]
def map_asset(asset, total_asset):
    rarity = handle_rarity(asset["traits"], total_asset)
    price = handle_price(asset["last_sale"])
    return [rarity, price]

def handle_rarity(traits, total_asset):
    rarity = None
    for i in traits:
        if rarity:
            rarity *= i["trait_count"] / total_asset
        else:
            rarity = i["trait_count"] / total_asset
    if rarity:
        return rarity * 100 
    return 100

def handle_price(price):
    if price:
        return int(price["total_price"]) / 10.**18
    return 0 

n = len(result)
res = [map_asset(i, n) for i in result]
arr = np.array(res)
df = pd.DataFrame(arr, columns=["rarity", "price"])
df

#%%

from sklearn.model_selection import train_test_split
train , test = train_test_split(df, test_size = 0.2) # using a test-size of 0.2 

x_train = train.drop('rarity', axis=1)
y_train = train['price']

x_test = test.drop('rarity', axis = 1)
y_test = test['price']
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

x_train_scaled = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train_scaled)

x_test_scaled = scaler.fit_transform(x_test)
x_test = pd.DataFrame(x_test_scaled)

#%%

# Getting the (Root Mean Squared) deviation for different models:
rmse_val_modela0 = [] # linear regression 
rmse_val_modela1 = [] # non weighted 
rmse_val_modela2 = [] # weighted on distance


print("-------- Model 0 : Linear Model ---------")
    
# Using a KNN Regression inverse distance normal model 
model1 = linear_model.LinearRegression()  
model1.fit(x_train, y_train) 
pred=model1.predict(x_test) 
error = sqrt(mean_squared_error(y_test,pred)) 
rmse_val_modela0.append(error) 
print('(Model 0) Deviation', 'is:', error)
    


print("-------- Model 1 : Normal Weighting ---------")
    
# Using a KNN Regression inverse distance normal model 
for K in range(30):
    K = K+1
    model1 = neighbors.KNeighborsRegressor(n_neighbors = K)  
    model1.fit(x_train, y_train) 
    pred=model1.predict(x_test) 
    error = sqrt(mean_squared_error(y_test,pred)) 
    rmse_val_modela1.append(error) 
    print('(Model 1) Deviation for k= ' , K , 'is:', error)
    
print("-------- Model 2 : Distance Weighting ---------")
    
# Using a KNN Regression inverse distance weighted model 
for K in range(30):
    K = K+1
    model1 = neighbors.KNeighborsRegressor(n_neighbors = K, weights="distance")  # quite 
    model1.fit(x_train, y_train)  
    pred=model1.predict(x_test) 
    error = sqrt(mean_squared_error(y_test,pred)) 
    rmse_val_modela2.append(error) 
    print('(Model 2) Deviation for k=' , K , 'is:', error)

#%%

curve0 = pd.DataFrame([rmse_val_model0 for _ in range(30)]) # single point 
curve1 = pd.DataFrame(rmse_val_model1) # elbow curve  with the best results at k = 7 ?
curve2 = pd.DataFrame(rmse_val_model2) # logarithmic curve with best at k = 1 

curve0.plot()
curve1.plot()
curve2.plot()

#%%

model = neighbors.KNeighborsRegressor(n_neighbors = 10, weights="distance") 
model.fit(x_train, y_train)  
model.predict(np.array([[i/1000] for i in range(1000)]))

#%% md

# Data Collection - Collection Names

Okay, so we have methods to collect asset information from the collections, but we don't have collection information.

Each event has a key "asset_events" which returns an array of all the events. Inside each of these events, there is an attribute called "asset", which contains information about each asset in the event.  

#%%

# Collecting one event information 
import requests
import json

url = "https://api.opensea.io/api/v1/events?only_opensea=false&offset=0&limit=1"

headers = {"Accept": "application/json"}

response = requests.request("GET", url, headers=headers)
res = json.loads(response.text)["asset_events"]
print(res[0].keys())
print(json.loads(response.text)["asset_events"][0]["collection_slug"]) # ["slug"]

#%%



#%%



#%%

# Collecting all the collections
collections = {}
url = "https://api.opensea.io/api/v1/events"

offset = 0
#while True:
if True:
    querystring = {
        "offset":offset,
        "limit":"300",
    }
    headers = {"Accept": "application/json"}

    response = requests.request("GET", url, headers=headers, params=querystring)
    re = json.loads(response.text)["asset_events"]
    for i in re:
        slug = i["collection_slug"]
        if not (slug in collections):
            collections[slug] = True
    offset += 300 

#%%

collections

#%% md

The next step is to apply the asset collection for all of these collection.

#%% md

# Putting it all together 
Combining everything for a price recommendation for individual NFTs

#%%

# Methods

# ----- Data Collection -----

# Takes in an asset response and returns a dictionary with relevant data points 
def isolate_parameters(asset: dict) -> dict:
    return {
            "num_sales":asset["num_sales"], 
            "name": asset["name"], 
            "description": asset["description"],
            "traits": asset["traits"], 
            "last_sale": asset["last_sale"], 
            "top_bid": asset["top_bid"],
            "sell_orders": asset["sell_orders"],
            "image_url": asset["image_url"]
           }

# Collecting assets from collection 
def collect_asset_information(collection_name = "cryptopunks" : str, isolate_parameters=isolate_parameters ):
    url = "https://api.opensea.io/api/v1/assets"
    result = []

    for i in range(1000):
        try:
            querystring = {
                "token_ids": list(range((i*29)+1, (i*29)+30)),
                "order_direction":"asc",
                "offset":"0",
                "limit":"50",
                "collection":collection_name,
            }
            headers = {"Accept": "application/json"}

            response = requests.request("GET", url, headers=headers, params=querystring)
            re = json.loads(response.text)
            if re["assets"] == []:
                break;
            for i in re["assets"]:
                # Populate database 
                result.append(isolate_parameters(i))
        except: 
            break;
        
    return result

# Collect all collection slugs 
def collect_collection_information(all_collection : dict, offset = 0 : int):
    url = "https://api.opensea.io/api/v1/events"
    collection = []
    querystring = {
        "offset":offset,
        "limit":"300",
    }
    headers = {"Accept": "application/json"}

    response = requests.request("GET", url, headers=headers, params=querystring)
    re = json.loads(response.text)["asset_events"]
    for i in re:
        slug = i["collection_slug"]
        if not (slug in all_colelctions):
            collections.append(slug)
            all_collections[slug]
    offset += 300 
    return collections

# ----- Data Pre-Processing -------

# Converting assets to [rarity, price] for data 
def map_asset(asset : dict, total_asset : int):
    rarity = handle_rarity(asset["traits"], total_asset)
    price = handle_price(asset["last_sale"])
    return [rarity, price]

# calculating rarity percentage ()
def handle_rarity(traits : dict, total_asset :int) -> float:
    rarity = None
    for i in traits:
        if rarity:
            rarity *= i["trait_count"] / total_asset
        else:
            rarity = i["trait_count"] / total_asset
    if rarity:
        return rarity * 100 
    return 100

# Determines the last sale price for the model
def handle_price(price : dict) -> int :
    if price:
        return int(price["total_price"]) / 10.**18
    return 0 


# ----- Model Initialisation 
def create_model(asset_information : list):
    


result = collect_asset_information()
